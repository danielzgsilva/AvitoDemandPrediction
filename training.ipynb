{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.join(os.getcwd(), 'datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features = pd.read_csv(os.path.join(data_root, 'engineered_features.csv'))\n",
    "train_orig = pd.read_csv(os.path.join(data_root, 'train.csv'))\n",
    "test_orig = pd.read_csv(os.path.join(data_root, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_orig.merge(engineered_features, on='user_id', how='left')\n",
    "test_df = test_orig.merge(engineered_features, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.head(50).drop(['item_id', 'user_id'], axis=1)\n",
    "test_df = test_df.head(50).drop(['item_id', 'user_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_cols = list(engineered_features.columns)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [\n",
    "    'image_top_1', 'param_1', 'param_2', 'param_3', \n",
    "    'city', 'region', 'category_name', 'parent_category_name', 'user_type'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df, categories):\n",
    "    # Fill missing values\n",
    "    df['description'].fillna('unknowndescription', inplace=True)\n",
    "    df['title'].fillna('unknowntitle', inplace=True)\n",
    "    \n",
    "    df['price'].fillna(df['price'].mean(), inplace=True)\n",
    "    df['image'].fillna('noimage', inplace=True)\n",
    "    \n",
    "    for col in agg_cols:\n",
    "        df[col].fillna(-1, inplace=True)\n",
    "        \n",
    "    for col in categorical:\n",
    "        df.loc[:, col] = df[col].fillna('').astype(str)\n",
    "    \n",
    "    # Engineer weekday feature\n",
    "    df['weekday'] = pd.to_datetime(df['activation_date']).dt.day.fillna(0)\n",
    "    df['month_num'] = pd.to_datetime(df['activation_date']).dt.day.fillna(0)\n",
    "    df.drop(['activation_date'], axis=1, inplace=True)\n",
    "    \n",
    "    # Count number of words and unique words in text fields\n",
    "    for col in ['description', 'title']:\n",
    "        df['num_words_' + col] = df[col].apply(lambda comment: len(comment.split())).fillna(0)\n",
    "        df['num_unique_words_' + col] = df[col].apply(lambda comment: \n",
    "                                                      len(set(w for w in comment.split()))).fillna(0)\n",
    "    \n",
    "    # Compute ratio  of words to unique words\n",
    "    df['words_vs_unique_title'] = (df['num_unique_words_title'] / \n",
    "                                   df['num_words_title'] * 100).fillna(0)\n",
    "    df['words_vs_unique_description'] = (df['num_unique_words_description'] / \n",
    "                                         df['num_words_description'] * 100).fillna(0)\n",
    "    \n",
    "    # TF-IDF \n",
    "    title_vectorizer = CountVectorizer(stop_words=stopwords.words('russian'), lowercase=True)\n",
    "    \n",
    "    desc_vectorizer = TfidfVectorizer(stop_words=stopwords.words('russian'), \n",
    "                                            lowercase=True, ngram_range=(1, 2),\n",
    "                                            max_features=15000)\n",
    "    \n",
    "    title_vecs = title_vectorizer.fit_transform(df['title'])\n",
    "    desc_vecs = desc_vectorizer.fit_transform(df['description'])\n",
    "\n",
    "    title_vecs = pd.DataFrame(title_vecs.todense(), columns=title_vectorizer.get_feature_names())\n",
    "    desc_vecs = pd.DataFrame(desc_vecs.todense(), columns=desc_vectorizer.get_feature_names())\n",
    "    \n",
    "    # one hot encoding\n",
    "    encoder = OneHotEncoder(drop='first')\n",
    "    \n",
    "    encoded_vecs = encoder.fit_transform(df[categories + ['weekday', 'month_num']])\n",
    "    encoded_vecs = pd.DataFrame(encoded_vecs.todense(), columns=encoder.get_feature_names())\n",
    "    \n",
    "    df.drop(categories+['description', 'title'], axis=1, inplace=True)\n",
    "    \n",
    "    df = pd.concat([df, title_vecs, desc_vecs, encoded_vecs], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = transform(train_df, categorical)\n",
    "test_df = transform(test_df, categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(train_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape) \n",
    "print(valid_df.shape) \n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvitoDataset(Dataset):\n",
    "    \"\"\"Avito Torch dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        df = df.reset_index()\n",
    "        self.images = df['image']\n",
    "        self.deal_probs = df['deal_probability']\n",
    "        self.features = df.drop(['deal_probability', 'image'], axis=1)\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_name = os.path.join(self.data_dir, self.images[idx])\n",
    "        \n",
    "        #image = io.imread(img_name)\n",
    "       \n",
    "        features = torch.tensor(self.features.iloc[idx])\n",
    "        features = features.view(1, 1, -1)\n",
    "        gt = torch.tensor(self.deal_probs[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return features, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_workers = 8\n",
    "num_features = train_df.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = AvitoDataset(train_df, data_root)\n",
    "valid = AvitoDataset(valid_df, data_root)\n",
    "\n",
    "datasets = {'Train': train, 'Validation': valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x: DataLoader(datasets[x], batch_size=batch_size, shuffle=True, num_workers = num_workers)\n",
    "              for x in ['Train', 'Validation']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're defining what component we'll use to train this model\n",
    "# We want to use the GPU if available, if not we use the CPU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(num_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.block2 =  nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # Mimic the second block here, except have this block extract 128 features\n",
    "        self.fc =  nn.Linear(512, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(x.size())\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "epochs = 10\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(epoch, model, dataloaders, device, phase):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    if phase == 'Train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    # Looping through batches\n",
    "    for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "    \n",
    "        # ensures we're doing this calculation on our GPU if possible\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Calculate gradients only if we're in the training phase\n",
    "        with torch.set_grad_enabled(phase == 'Train'):\n",
    "      \n",
    "            # This calls the forward() function on a batch of inputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss of the batch\n",
    "            loss = criterion(outputs, labels)\n",
    "            rmse = np.sqrt(loss.item())\n",
    "\n",
    "            # Adjust weights through backpropagation if we're in training phase\n",
    "            if phase == 'Train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Document statistics for the batch\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_rmse += rmse * inputs.size(0)\n",
    "    \n",
    "    # Calculate epoch statistics\n",
    "    epoch_loss = running_loss / datasets[phase].__len__()\n",
    "    epoch_acc = running_rmse / datasets[phase].__len__()\n",
    "\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_epochs, dataloaders, device):\n",
    "    start = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    print('| Epoch\\t | Train Loss\\t| Train Acc\\t| Valid Loss\\t| Valid Acc\\t| Epoch Time |')\n",
    "    print('-' * 86)\n",
    "    \n",
    "    # Iterate through epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        epoch_start = time.time()\n",
    "       \n",
    "        # Training phase\n",
    "        train_loss, train_acc = run_epoch(epoch, model, dataloaders, device, 'Train')\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = run_epoch(epoch, model, dataloaders, device, 'Validation')\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "           \n",
    "        # Print statistics after the validation phase\n",
    "        print(\"| {}\\t | {:.4f}\\t| {:.4f}\\t| {:.4f}\\t| {:.4f}\\t| {:.0f}m {:.0f}s     |\"\n",
    "                      .format(epoch + 1, train_loss, train_acc, val_loss, val_acc, \n",
    "                              epoch_time // 60, epoch_time % 60))\n",
    "\n",
    "        # Copy and save the model's weights if it has the best accuracy thus far\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    \n",
    "    print('-' * 74)\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(total_time // 60, total_time % 60))\n",
    "    print('Best validation accuracy: {:.4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights and return them\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(model, criterion, optimizer, epochs, dataloaders, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
